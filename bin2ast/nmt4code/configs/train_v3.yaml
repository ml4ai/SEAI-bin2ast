# config file to train our models in kraken
# location of pickled data
pickled_data_path: "/home/kcdharma/data/nmt4code/pickled_data/v3"
# hidden dimension size for encoder
enc_hidden_dim: 512
# hidden dimension size for decoder
dec_hidden_dim: 512
# input language: in our case assembly on the input side
input_lang: "token_assembly"
# output language: in our case abstract syntax trees
output_lang: "token_CAST"
# gpu index to train on if available
gpu_index: 0
# batch size for training/validation/testing
batch_size: 32
# embedding dimension for input
input_embedding_dim: 256
# number of layers for lstm
n_layers: 1
# dropout to use for lstm
dropout: 0.5
# embedding dimension for output
output_embedding_dim: 256
# learning rate for the optimizer [encoder and decoder]
learning_rate: 0.01
# num epochs to train on
num_epochs: 100
# save model to following location
save_path: "/home/kcdharma/results/nmt4code/v3"
# clip the gradiet to max norm of
clip: 1
# uncomment just one model_type for training
# model type: for basic enc_dec
# model_type: "basic"
# model type: for enc_dec with attention
# model_type: "attention"
# model_type: for transformer
model_type: "transformer"
# hidden dimension of the transformer: d_model
transformer_hidden_dim: 256
# transformer number of heads
transformer_n_heads: 8
# encoder number of layers
transformer_enc_layers: 3
# decoder number of layers
transformer_dec_layers: 3
# position feedforward dimensions
transformer_pf_dim: 512
# encoder dropout value
transformer_enc_dropout: 0.1
# decoder dropout value
transformer_dec_dropout: 0.1
# max length transformer can handle
transformer_max_length: 2000
# learning rate for transformer
transformer_lr: 0.0005
